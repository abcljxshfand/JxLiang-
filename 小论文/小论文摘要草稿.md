# 小论文摘要草稿

现有的跨模态行人重识别方法大多选择模态互转或直接将特征映射到共同特征空间，主要关注于缓解模态差异，而忽略同一模态下的类间差异，这不利于模型学习到具有辨别性的身份特征。本文提出一种基于改进的多头自注意力机制的非共享双流网络，分别提取可见光图像和红外图像的特征，并对特征做水平划分，使用局部特征来计算身份损失以及异质中心损失，利用多头自注意力机制根据像素本身的内容及其相对于相邻像素的空间位置来计算像素的注意力图，增强模型提取的信息容量，进而学习到同模态下不同类之间具有辨别性的身份特征。同时使用辅助模态来减缓可见光图像和红外图像在颜色信息上的差异，进一步减小模态差异。在SYSU-MM01和RegDB数据集上的mAP分别达到60.36%和76.10%，实验表明该方法的有效性。





跨模态行人重识别旨在匹配来自不同模态的相同身份的行人图像。现有方法主要通过生成对抗网络进行模态特定信息的补偿或直接提取原始图像的模态共享特征来缓解模态差异。但是，进行模态特定信息补偿通常需要复杂的生成器和判别器，并且生成的伪图像与真实图像之间仍然存在一定的差距，且容易引入噪声。同时，由于可见光图像和红外图像之间存在模态差异，直接提取两种模态的模态共享特征十分困难，而且不能充分挖掘细微的、更具鉴别性的特征信息。



可见红外行人重新识别（VI-ReID）旨在跨不同光谱搜索行人的身份。在这项任务中，主要挑战之一是可见光 (VIS) 和红外 (IR) 图像之间的模态差异。一些最先进的方法尝试设计复杂的网络或生成方法来减轻模态差异，同时忽略 VIS 和 IR 两种模态之间的高度非线性关系。在本文中，我们提出了一种非线性中间模态生成器（MMG），这有助于减少模态差异。我们的MMG可以有效地将可见光和红外图像投影到统一的中间模态图像（UMMI）空间中，以生成中间模态（M-modality）图像。

生成的 M 模态图像和原始图像被输入骨干网络以减少模态差异。

此外，为了将 UMMI 空间中的 VIS 和 IR 图像生成的两种类型的 M 模态图像结合在一起，我们提出了分布一致性损失（DCL），以使生成的 M 模态图像的模态分布尽可能一致。最后，我们提出了一种中间模态网络（MMN），以显式的方式进一步增强特征的辨别力和丰富性。我们进行了大量的实验，以验证 MMN 在两个具有挑战性的数据集上的 VI-ReID 相对于一些最先进方法的优越性。即使与 SYSUMM01 数据集上最新的最先进方法相比，MMN 在 Rank-1 和 mAP 方面的增益也分别超过 11.1% 和 8.4%。



可见光-红外行人重识别（Re-ID）旨在匹配来自不同模态的相同身份的行人图像。现有的工作主要集中在通过对齐不同模态的特征分布来减轻模态差异。然而，诸如眼镜、鞋子和衣服长度等细微但有区别的信息尚未得到充分探索，特别是在红外模式中。在不发现细微差别的情况下，仅使用模态对齐来匹配跨模态的行人是一项挑战，这不可避免地会降低特征的独特性。在本文中，我们提出了一种联合模态和模式**对齐网络**（MPANet）来发现可见红外行人重新识别的不同模式中的跨模态细微差别，该网络引入了模态缓解模块和模式对齐模块来联合提取判别特征。具体来说，我们首先提出一个模态减轻模块，以从提取的特征图中去除模态信息。然后，我们设计了一个模式对齐模块，它为一个人的不同模式生成多个模式图，以发现细微差别。最后，我们引入了一种相互均值学习方式来减轻模态差异，并提出了一种中心簇损失来指导身份学习和细微差别发现。对公共 SYSU-MM01 和 RegDB 数据集的大量实验证明了 MPANet 相对于最先进技术的优越性













# 专业术语和常见句式

we divide these models into four categories, including **modality-shared feature learning**, **modality-specific information compensation,** auxiliary information and data augmentation.





Visible-infrared person re-identification (VI-ReID) aims to search identities of pedestrians across different spectra. In this task, **one of the major challenges is the modality discrepancy between the visible (VIS) and infrared (IR) images.**





**we propose a non-linear middle modality generator (MMG), which helps to reduce the modality discrepancy. Our MMG can effectively project VIS and IR images into a unified middle modality image (UMMI) space to generate middle-modality (M-modality) images.**

The generated M-modality images and the original images are fed into the backbone network to reduce the modality discrepancy.



**In this paper, we propose a joint Modality and Pattern Alignment Network (MPANet) to discover cross-modality nuances in different patterns for visibleinfrared person Re-ID, which introduces a modality alleviation module and a pattern alignment module to jointly extract discriminative features**. Specifically, **we first propose a modality alleviation module to dislodge the modality information from the extracted feature maps.** 



**cross modal pedestrian re identification**



Extensive experiments on the public SYSU-MM01 and RegDB datasets demonstrate the superiority of MPANet over state-of-the-arts.