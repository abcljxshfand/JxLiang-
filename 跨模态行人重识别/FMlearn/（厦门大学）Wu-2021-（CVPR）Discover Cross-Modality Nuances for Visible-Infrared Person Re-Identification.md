# （厦门大学）Wu-2021-（CVPR）Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification

**2022.11.25：**

**（注：棕色标注表示我的一些疑问；黄色标注表示阅读文献时候的注意点，帮助我重新读的时候更快理解文章内容；绿色标注表示问题（研究目标、研究内容、关键问题），问题背景等；蓝色标注表示某项方法、行为的作用、目的、意义；红色标注表示我认为的重点，需要特别关注；紫色标注表示预备知识；）**

**（注：现阶段（基础），笔记主要围绕三点，1.知识体系结构  2.文章的诉求  3.文章的主要贡献、特点、创新点。）**

![image-20221106105213097](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20221106105213097.png)

------



## 背景、目的与结论

### **背景**

在跨模态行人重识别任务中，最关键的是要解决模态间的差异。

不同图像对的细微差别出现在不同的模式中，例如T恤和裤子的长度、鞋子的类型以及是否戴眼镜。

尽管已经有了很多方法，并取得了一定的成效。但是对图像间的细微差异挖掘得不够充分，现有方法中学习到具有鉴别性的特征的能力仍有待提高。

（**注：**已经提出了相当多的细粒度的人重新识别方法[15，24，27，36，40，43]，它主要将身份分类、人辅助信息合并到一个框架中，以考虑人的细节。**<font color='red'>然而，这些方法需要额外的标记先验，例如属性、关键点和人类解析信息，寻找某些部分并平等对待这些部分，而不是自适应地选择它们。由于缺乏必要的信息和模态的变化，这些方法无法在跨模态环境中学习辨别特征</font>**。因此，发现现有方法中未充分利用的细微差别自然可以提高特征的辨别能力。）



### **目的**

设计一个模型

1. 缓解模态间的差异
2. 同时可以发现可见红外人物Re-ID的不同模式中的跨模态细微差别，提取出具有鉴别性的特征。



### **结论**

<img src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20221125213154814.png" alt="image-20221125213154814" style="zoom:200%;" />

1. 提出了一种**联合模态和模式对齐网络（a joint Modality and Pattern Alignment Network，MPANet）**来发现可见红外人物Re-ID的不同模式中的跨模态细微差别，该网络引入了模态减轻模块和模式对齐模块来联合提取辨别特征。
2. 具体地，我们首先提出了一种**模态减轻模块（The Modality Alleviation Module，MAM）**，以从提取的特征图中去除模态信息。
3. 然后，我们设计了一个**模式对齐模块（the Pattern Alignment Module，PAM）**，它为一个人的不同模式生成多个模式图，以发现细微差别。
4. 最后，我们引入了**一种互均值学习方式（a mutual mean learning fashion ）**来缓解模态差异，并提出了一种**中心聚类损失（ the cross-entropy loss）**来指导**身份学习和细微差别发现（identity learning and nuances discovering）**。在公共SYSU-MM01和RegDB数据集上的大量实验表明MPANet优于现有技术。



### 相关工作





## 方法



## 结果与讨论

### 数据集介绍

**SYSU-MM01**

SYSU-MM01包含由6台摄像机拍摄的图像，包括**2台红外摄像机和4台RGB摄像机**。

在SYSU-MM01数据集中有491个有效的ID。我们有一个固定的分割，使用296个身份进行训练，99个身份进行验证，96个身份进行测试。在训练阶段，训练集中296个身份在所有相机中的所有图像都可以被应用。
**<font color='red'>在测试阶段，来自RGB相机的样本用于gallery集，来自IR相机的样本用于probe集。</font>**
我们设计了两种模式，**全搜索模式和室内搜索模式**。在全搜索模式下，RGB相机1、2、4和5用于gallery集，红外相机3和6用于probe集。对于室内搜索模式，RGB相机1和2（不包括室外相机4和5）用于gallery集，红外相机3和6用于probe集。
对于这两种模式，我们采用单次拍摄（single-shot）和多次拍摄(multi-shot)的设置。对于RGB相机下的每一个身份，我们随机选择该身份的一/十张图像组成单张/多张设置的gallery集。对于probe集，所有的图像都被使用。给定一个probe图像，通过计算probe图像和gallery图像之间的相似性来进行匹配。请注意，匹配是在不同位置的相机之间进行的（位置如表2所示）。摄像机2和摄像机3在同一地点，所以摄像机3的probe图像跳过了摄像机2的gallery图像。计算完相似度后，我们可以根据相似度的降序得到一个排名表。
为了表示性能，我们使用累积匹配特性（CMC）[32]和平均精度（mAP）。请注意，对于多镜头设置下的CMC，只取同一人的所有gallery图像中的最大相似度来计算排名列表。我们用随机分割的gallery集和probe集重复上述评估10次，最后计算出平均性能。

![image-20221220202414423](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20221220202414423.png)

![image-20221125215455615](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20221125215455615.png)



![image-20221125215517414](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20221125215517414.png)

## 文章好在哪里

### 



**主要贡献：**

1. 我们在统一的框架中**<font color='red'>解决了可见红外人员Re-ID的细微差别发现和模态差异</font>**。文献中没有对前者进行探讨，而后者是跨模式匹配人的关键。
2. 为了发现细微差别并提取辨别特征，提出了**模式对齐模块（PAM）**，以无监督的方式发现不同模式中的细微差别，并提出了中心聚类损失和分离损失。
3. 为了在保持身份信息的同时缓解模态差异，提出了**模态缓解模块（MAM）**，该模块在相互平均学习方式的指导下选择性地应用实例归一化。

## 自我思考

### 思路与问题

1.

本文中提到**<font color='red'>”现有的工作主要集中于通过对齐不同模态的特征分布来缓解模态差异（Existing works mainly focus on alleviating the modality discrepancy by aligning the distributions of features from different modalities）“</font>**如何理解？



2.

本文的创新点是怎么来的？



3.

如何理解本文的标题：**<font color='red'>Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification</font>**中的细微差别（the nuances）

**答：**

可见光图像和红外图像相比而言，可见光图像具有更多样的鉴别性特征，如颜色信息、纹理信息等等，这些特征是红外图像所没有的，这就导致了不同身份行人的红外图像比较难以分辨，而可见光图像易于区分。同时，同一行人两种模态的图像是很大的差异的。

目前的工作侧重点是放在通过模态对齐或者像素对齐来减缓模态间的差异。尽管取得了不错的成果，但是在红外图像上仍存在许多有效信息尚待挖掘，因此现有工作在发掘跨模态鉴别性的特征这一方面仍然有限。

在 cross-modality person Re-ID 中，不同图像对中的细微差别以各种模式出现，例如 T 恤和裤子的长度、鞋子的类型以及是否戴眼镜。如果没有很好地发现这些信息，红外特征的可辨别性将比可见特征差。



4.

是在哪里挖掘细微差异？如何挖掘细微差异？挖掘了细微差异后怎么用？







### 句式积累

创新点：

为了怎么怎么样，提出了什么什么模块，解释这个模块的作用，并且还用了什么什么技术。

研究内容：

提出了一个怎么怎么样的模型，这个模型是什么样子的，效果怎么样

设计了什么样的模块，它的作用是什么

用了哪种优化方法，达到了什么样的目的。同时用了哪种度量函数，它的作用是什么。