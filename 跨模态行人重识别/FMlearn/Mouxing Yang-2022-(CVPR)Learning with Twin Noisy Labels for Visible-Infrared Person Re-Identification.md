# Mouxing Yang-2022-(CVPR)Learning with Twin Noisy Labels for Visible-Infrared Person Re-Identification

# 摘要





# 引言

**存在的问题：**

尽管 VI-ReID 取得了令人鼓舞的性能，但其成功在很大程度上依赖于高质量的注释数据。

然而，在实践中，由于可识别性差，**很难甚至不可能精确地注释所有样本**，尤其是颜色信息在红外模态中丢失，如图 1(a) 所示。结果，不可避免地会导致**噪声注释（NA）问题**，从而降低 ReID 模型的性能。

**噪声注释：**由于红外模态的可识别性差，身份 1 和 2 的样本 2 将被混淆，从而分别被**错误地注释**为身份 2 和 1

**噪声对应：**由于跨模态对是借助注释构建的，正负对都可能由于噪声注释而为假，从而导致不匹配现象。在这种噪声对应关系中，假阳性和假阴性会在训练过程中分别被错误地拉动和推入。

![image-20230612152817938](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20230612152817938.png)



**解决问题的思路：**

基于上述观察，在本文中，我们揭示了 VI-ReID 的一个新问题，称为双噪声标签 (TNL，Twin Noisy Labels)。

与仅考虑 NA 挑战的传统噪声标签研究 [6、6、10、12、16] 不同，**TNL 同时考虑类别中的 NA （ Noisy Annotations）和跨模态对中的 NC（ Noisy Correspondence ）**。需要指出的是，采用现有的面向 NA 的方法来纠正 VI-ReID 中的噪声标注以解决 TNL 问题是很棘手的，原因如下。（略，编故事，写论文的时候可以学习）



**解决方案：**

为了解决 VI-ReID 中的 TNL 问题，我们提出了一种同时使用噪声注释和对应进行学习的新方法。拟议的 **DuAlly Robust Training (DART)** 由具有**新颖目标函数**的**联合建模**和对分模块组成。

具体来说，**协同建模模块**首先利用深度神经网络的记忆效应计算每个样本的干净置信度。

然后 **pair-division 模块**用置信度校正噪声对应关系，进一步将噪声对分为四个子集，即真阳性对 (TP)、真阴性对 (TN)、假阳性对 (FP) 和假阴性对(FN)。

最后，为了实现稳健的 VI-ReID，我们提出了一种**新颖的双稳健目标函数**，它由软识别损失和自适应四元组损失组成。简而言之，软识别损失用于在学习身份感知表示时惩罚噪声注释样本。自适应四元组损失利用上述四种对来减轻模态差异



**主要贡献（总结，编故事，写论文看）**





# 方法

![image-20230612154908010](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20230612154908010.png)